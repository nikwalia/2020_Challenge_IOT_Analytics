{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60oNtF4FWakJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "import random\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Activation, MaxPooling1D, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9A1LKB0GwtQH"
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #model will be trained on GPU 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCvn51nkpuah"
   },
   "outputs": [],
   "source": [
    "              \"\"\"Hyperparameters\"\"\"\n",
    "w = 17280-500                 # History window (number of time stamps taken into account) \n",
    "                         # i.e., filter(kernel) size       \n",
    "p_w = 500                # Prediction window (number of time stampes required to be \n",
    "                         # predicted)\n",
    "n_features = 1           # Univariate time series\n",
    "\n",
    "kernel_size = 2          # Size of filter in conv layers\n",
    "num_filt_1 = 32          # Number of filters in first conv layer\n",
    "num_filt_2 = 32          # Number of filters in second conv layer\n",
    "num_nrn_dl = 40          # Number of neurons in dense layer\n",
    "num_nrn_ol = p_w        # Number of neurons in output layer\n",
    "\n",
    "conv_strides = 1\n",
    "pool_size_1 = 2          # Length of window of pooling layer 1\n",
    "pool_size_2 = 2          # Length of window of pooling layer 2\n",
    "pool_strides_1 = 2       # Stride of window of pooling layer 1\n",
    "pool_strides_2 = 2       # Stride of window of pooling layer 2\n",
    "\n",
    "epochs = 30\n",
    "dropout_rate = 0.5       # Dropout rate in the fully connected layer\n",
    "learning_rate = 2e-5  \n",
    "anm_det_thr = 0.8        # Threshold for classifying anomaly (0.5~0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "colab_type": "code",
    "id": "aKYzBd94KZFu",
    "outputId": "52cfaed6-92e1-42ab-8ce8-9e4af7cf69b2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdf_helper import *\n",
    "from stat_helper import *\n",
    "from data_cleaning import *\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('data/datch_3.csv').drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "df_test = pd.read_csv('data/dat_ch_3_no_outliers.csv', nrows=100)\n",
    "\n",
    "float_cols = [c for i, c in enumerate(df_test.columns) if i != 0]\n",
    "float64_cols = {c: np.float64 for c in float_cols}\n",
    "\n",
    "df = pd.read_csv('data/datch_3.csv', engine='c', dtype=float64_cols).drop(['Unnamed: 0'], axis = 1)\n",
    "df = df.replace(np.NAN, 0.0)\n",
    "\n",
    "zero_outliers = df.loc[:, (df == 0.0).all(axis=0)]\n",
    "reg_data = df.loc[:,(df != 0.0).any(axis=0)]\n",
    "\n",
    "#df = reduce_dataset_size(df, cluster_size = 50)\n",
    "df = smooth_values(df)\n",
    "scaler = RobustScaler()\n",
    "df = pd.DataFrame(scaler.fit_transform(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = len(df.index) - 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQsOxuDCIGpQ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preprocessing\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + w\n",
    "        out_end_ix = end_ix + p_w\n",
    "        # check if we are beyond the sequence\n",
    "        if out_end_ix > len(sequence):\n",
    "            break\n",
    "    # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(27, 1, 16780)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smoor\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  import sys\n",
      "C:\\Users\\smoor\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# # define input sequence\n",
    "# for col in reg_data.columns:\n",
    "#     sampl, labl = split_sequence(list(reg_data[col]))\n",
    "samples = []\n",
    "labels = []\n",
    "\n",
    "batch_sampl, batch_labl = split_sequence(list(reg_data.ix[:,0]))\n",
    "samples.append(batch_sampl)\n",
    "labels.append(batch_labl)\n",
    "\n",
    "print()\n",
    "\n",
    "for i in range(1, len(reg_data.columns)):\n",
    "    batch_sampl, batch_labl = split_sequence(list(reg_data.ix[:,i]))\n",
    "    samples.append(batch_sampl)\n",
    "    labels.append(batch_labl)\n",
    "    \n",
    "batch_sample = np.array(samples)\n",
    "batch_label = np.array(labels)\n",
    "    \n",
    "print(batch_sample.shape)\n",
    "\n",
    "# summarize the data\n",
    "# for i in range(5):\n",
    "#     print(X[i], Y[i])\n",
    "  \n",
    "# 2. reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "\n",
    "# need to convert batch into 3D tensor of the form [batch_size, input_seq_len, n_features]\n",
    "batch_sample = batch_sample.reshape((batch_sample.shape[0], batch_sample.shape[2], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 1, 500)\n",
      "(27, 16780, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_label = batch_label.reshape((batch_label.shape[0],batch_label.shape[1],batch_label.shape[2]))\n",
    "print(batch_label.shape)\n",
    "print(batch_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "sLBqHZLmomSD",
    "outputId": "375909cf-8a14-4e10-aa93-7d5d2fac440a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_38 (Conv1D)           (None, 16779, 32)         96        \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 8389, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_39 (Conv1D)           (None, 8388, 32)          2080      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 4194, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 134208)            0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 40)                5368360   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 500)               20500     \n",
      "=================================================================\n",
      "Total params: 5,391,036\n",
      "Trainable params: 5,391,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "              \"\"\"Generate model for predictor\"\"\"\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layer #1\n",
    "# Computes 32 features using a 1D filter(kernel) of with w with ReLU activation. \n",
    "# Padding is added to preserve width.\n",
    "# Input Tensor Shape: [batch_size, w, 1] / batch_size = len(batch_sample)\n",
    "# Output Tensor Shape: [batch_size, w, num_filt_1] (num_filt_1 = 32 feature vectors)\n",
    "model.add(Conv1D(filters=num_filt_1,\n",
    "                 kernel_size=kernel_size,\n",
    "                 strides=conv_strides,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 input_shape=(w, n_features)))\n",
    "\n",
    "# Pooling Layer #1\n",
    "# First max pooling layer with a filter of length 2 and stride of 2\n",
    "# Input Tensor Shape: [batch_size, w, num_filt_1]\n",
    "# Output Tensor Shape: [batch_size, 0.5 * w, num_filt_1]\n",
    "\n",
    "model.add(MaxPooling1D(pool_size=pool_size_1)) \n",
    "                    #  strides=pool_strides_1, \n",
    "                    #  padding='valid'))\n",
    "\n",
    "# Convolutional Layer #2\n",
    "# Computes 64 features using a 5x5 filter.\n",
    "# Padding is added to preserve width and height.\n",
    "# Input Tensor Shape: [batch_size, 0.5 * w, 32]\n",
    "# Output Tensor Shape: [batch_size, 0.5 * w, num_filt_1 * num_filt_2]\n",
    "model.add(Conv1D(filters=num_filt_2,\n",
    "                 kernel_size=kernel_size,\n",
    "                 strides=conv_strides,\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "\n",
    "# Max Pooling Layer #2\n",
    "# Second max pooling layer with a 2x2 filter and stride of 2\n",
    "# Input Tensor Shape: [batch_size, 0.5 * w, num_filt_1 * num_filt_2]\n",
    "# Output Tensor Shape: [batch_size, 0.25 * w, num_filt_1 * num_filt_2]\n",
    "model.add(MaxPooling1D(pool_size=pool_size_2))\n",
    "                    #  strides=pool_strides_2, \n",
    "                    #  padding='valid'\n",
    "          \n",
    "# Flatten tensor into a batch of vectors\n",
    "# Input Tensor Shape: [batch_size, 0.25 * w, num_filt_1 * num_filt_2]\n",
    "# Output Tensor Shape: [batch_size, 0.25 * w * num_filt_1 * num_filt_2]\n",
    "model.add(Flatten())\n",
    "\n",
    "# Dense Layer (Output layer)\n",
    "# Densely connected layer with 1024 neurons\n",
    "# Input Tensor Shape: [batch_size, 0.25 * w * num_filt_1 * num_filt_2]\n",
    "# Output Tensor Shape: [batch_size, 1024]\n",
    "model.add(Dense(units=num_nrn_dl, activation='relu'))  \n",
    "\n",
    "# Dropout\n",
    "# Prevents overfitting in deep neural networks\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "# Input Tensor Shape: [batch_size, 1024]\n",
    "# Output Tensor Shape: [batch_size, p_w]\n",
    "model.add(Dense(units=num_nrn_ol))\n",
    "\n",
    "# Summarize model structure\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "K76R-2HPajxI",
    "outputId": "b138835d-a0e7-4af7-a1fd-53b18446258a"
   },
   "outputs": [],
   "source": [
    "                 '''configure model'''\n",
    "model.compile(optimizer='adam', \n",
    "              loss='mean_absolute_error')\n",
    "\n",
    "# sgd = keras.optimizers.SGD(lr=learning_rate, \n",
    "#                          decay=1e-6, \n",
    "#                          momentum=0.9, \n",
    "#                          nesterov=True)\n",
    "# model.compile(optimizer='sgd', \n",
    "#               loss='mean_absolute_error', \n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MO8EMSBaMZ8K",
    "outputId": "7c033331-7d5e-473c-ac81-5c7971db6fa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nmodel_fit = model.fit(batch_sample,\\n                      batch_label,\\n                      epochs=epochs,\\n                      verbose=1)\\n'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "model_fit = model.fit(batch_sample,\n",
    "                      batch_label,\n",
    "                      epochs=epochs,\n",
    "                      verbose=1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 479ms/step - loss: 0.1092\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1727\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1107\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0186\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0092\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.0023\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.0022\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0021\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0028\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0017\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0015\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0012\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 9.9658e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 8.7294e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.4511e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 6.4059e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 6.1767e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.4507e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 6.5444e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 6.3574e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.0828e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 5.5047e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 4.8884e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.4244e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 4.2098e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.6904e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.1626e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.4772e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.3835e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.0678e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0499\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0209\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0214\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0016\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 7.1127e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 7.2640e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 6.8274e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 5.9648e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 4.9761e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.1213e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.6077e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.6260e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3.5092e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.2622e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.2652e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 3.1950e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0895e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.9127e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 2.7327e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 2.5910e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.4872e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.4475e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.3171e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.2425e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.2224e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.8841e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.7311e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.8768e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.9839e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.8943e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0031\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 2.3057e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0016\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.9511e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.9370e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.7044e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.2249e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.9365e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.9051e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.9223e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.9243e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.9514e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.9041e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.7231e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.6497e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.5956e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.5559e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.5471e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.6321e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.4824e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.3943e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.4891e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.4940e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.4450e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.3801e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.3423e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.3745e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.3368e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.3813e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.4071e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1020\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.0436\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.2283e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.6189e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4163e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.9465e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.3662e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.2019e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.2409e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.1871e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.1275e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.1221e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.0056e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.0189e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.0560e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.9012e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.7924e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.6914e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.6049e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.5599e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.6895e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.7082e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.6670e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.6218e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.3776e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2901e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.4052e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.5266e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.5105e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.3476e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.4005e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.4046e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.5152e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.4965e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3486e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2984e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2834e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.3964e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.4390e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3721e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3503e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.3137e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.3030e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2982e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.3258e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.3396e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.4481e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.4047e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.3654e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.3062e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2517e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2705e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3621e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3077e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1996e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.2207e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.3742e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.3800e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.3284e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2266e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 40.9086\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.9123e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.1244e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 14.5373\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 2.4731e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.4726e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.2108e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.8679e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.6020e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.5775e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.7837e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.8190e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.7999e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.7258e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.4970e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.4818e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.6042e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.5356e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.4561e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.4391e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3186e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3570e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.4639e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.4232e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.3851e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3431e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2901e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2399e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.3406e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.3906e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.5071\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5071\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.5070\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.5070\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5071\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5070\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5070\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5070\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5070\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5070\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5070\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5070\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5070\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5070\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5070\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5070\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5070\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5070\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5070\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5070\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5070\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5070\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5070\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5069\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5069\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5069\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5069\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5069\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5069\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 15.4714\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 15.3773\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 15.3773\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 15.3772\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 15.3771\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 15.3770\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 15.3769\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 15.3768\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 15.3767\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 15.3766\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 15.3764\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 15.3763\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 15.3761\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 15.3760\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 15.3758\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 15.3757\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 15.3755\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 15.3754\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 15.3752\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 15.3750\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 15.3749\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 15.3747\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 15.3745\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 15.3743\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 15.3742\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 15.3740\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 15.3738\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 15.3736\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 15.3735\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 15.3733\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 2.2245\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 2.2246\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 2.2247\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.2247\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.2247\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.2247\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.2247\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.2247\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.2246\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 2.2245\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.2244\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.2243\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.2242\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.2241\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.2239\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.2238\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.2237\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.2235\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.2234\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.2232\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.2230\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.2229\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.2227\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.2225\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.2223\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.2221\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.2219\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.2218\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.2216\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.2214\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.0022\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0021\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1132\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0018\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0017\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0015\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0013\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.0012\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.0010\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 9.2066e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 9.0041e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 9.4784e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0010\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0011\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0011\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0011\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.0011\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0010\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 9.9750e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 9.2293e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 8.3666e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 7.5427e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 6.7853e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 6.1206e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 5.7934e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5.7061e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5.7275e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5.7550e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 5.5819e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.1120e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4.5356e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4.0169e-04\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step - loss: 3.6719e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 3.4427e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 3.2113e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.9152e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6122e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.3052e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.1159e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.8832e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.8511e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.9157e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.9636e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.0021e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.0871e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.0900e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 2.0874e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.0736e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.9927e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.8801e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.8190e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.7794e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.6755e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.5426e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.5365e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.4693e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.4836e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.5801e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.6719e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.5749e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.4651e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3788e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.4530e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.4340e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.3239e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3024e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3562e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.3863e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2915e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2307e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2700e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2873e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3268e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.3131e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.2928e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.2611e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.3102e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2855e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2792e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2299e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1694e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2335e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2719e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.2450e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.2554e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1972e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.2061e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2395e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2357e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2124e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1980e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1345e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1012e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1913e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2658e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1678e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1507e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2176e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2709e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1999e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1661e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.1576e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1636e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1518e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1470e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.1907e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2197e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2479e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2553e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1690e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2131e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2535e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.2644e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2023e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1830e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1188e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1227e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1321e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2198e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1813e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1054e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1488e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1474e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1678e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1481e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1105e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1295e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1525e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.1942e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.2022e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2516e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2195e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2175e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.2207e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2278e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1827e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1825e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.1835e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2218e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1833e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1325e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1318e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2094e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2058e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1694e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1402e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1503e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2038e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2412e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1565e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1637e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1966e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1842e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2375e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.2561e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1611e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1574e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1672e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.2214e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1518e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1331e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1835e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2033e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1815e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.1268e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1224e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2050e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2117e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.2190e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1460e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1379e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1590e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2063e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1556e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1470e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1254e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.0703e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1750e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2186e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1926e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1850e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1526e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1825e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2568e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2507e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1667e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2258e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2470e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.2193e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1603e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1878e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1838e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1610e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1581e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1412e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1520e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1755e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1626e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.2230e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1047e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0907e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1705e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2332e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1862e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1461e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1468e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1596e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1918e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2171e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2142e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.2103e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1831e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1664e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1735e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2036e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1261e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1414e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1922e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2243e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.2030e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2042e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1455e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1284e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1795e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2046e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1856e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2170e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2343e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.2180e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1377e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1812e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1778e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1244e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1661e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1769e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1079e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.0997e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1606e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.3068e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2300e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1543e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1741e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2094e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1906e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1710e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1623e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1541e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2296e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.2186e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2034e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1760e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2310e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2733e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1704e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1855e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1689e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2172e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1706e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2085e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1681e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1192e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1376e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1789e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1658e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1584e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1752e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1355e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1489e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2006e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1896e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2025e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1394e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.1177e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2493e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2638e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1502e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1340e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1616e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2113e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1571e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.1259e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1221e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1760e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1715e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1178e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1002e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1536e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1775e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1700e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1612e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1565e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1665e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1923e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1645e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1854e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1709e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1449e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1849e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2403e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1721e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1348e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.1150e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1648e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2078e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1739e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1148e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1530e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2319e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2229e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1932e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2212e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1644e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1996e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2109e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.2253e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1719e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1137e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1364e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2038e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.1444e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.0446e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.1134e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1878e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2052e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.2158e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.0972e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.1325e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2011e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.2355e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.2016e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1835e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.1753e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1909e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2253e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2157e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1944e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.2267e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 1.2021e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.1973e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.2060e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1960e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1645e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2046e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1568e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1471e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1875e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.2524e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.2143e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.1490e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1342e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1447e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.1661e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.2137e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1922e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2058e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1245e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.0813e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.1317e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2250e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1826e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1313e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.1813e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.2067e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2469e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2437e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1423e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1983e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.2535e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2459e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.1687e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1811e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1890e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2269e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2038e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1570e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1736e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2170e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2446e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2355e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1357e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1562e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1757e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2885e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.2228e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1655e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.1866e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.1731e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.2240e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.2120e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1594e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1544e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2192e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1963e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1825e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1639e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1219e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1650e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1389e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1706e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1024e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1566e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.2094e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1822e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1422e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1042e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1545e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2171e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2031e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1785e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1718e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1478e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1185e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1604e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1741e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2179e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1836e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1429e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.1759e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.2447e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.2158e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1931e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1446e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1900e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2442e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2309e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1431e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1631e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.1696e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1012e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1104e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1613e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1806e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1564e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1256e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1425e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.1486e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.2132e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1610e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1895e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.1456e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.1372e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.1858e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.2157e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2010e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1839e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1588e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1452e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1993e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2176e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1756e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1525e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1358e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1539e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1230e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1737e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1248e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.0836e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1867e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.2108e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1485e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1835e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.2033e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2529e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.2156e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.1260e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.1724e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.2685e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2028e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1681e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1458e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1734e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2018e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.2699e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2135e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1424e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.1471e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.1394e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1355e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1674e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1453e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1354e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1238e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1118e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1364e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1460e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.0843e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1336e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1510e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1760e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2288e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.2267e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1928e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.2071e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.1851e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.2235e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.1694e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.1628e-04\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.2283e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.2598e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.2031e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.0897e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.0785e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2198e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2941e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2390e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.1327e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1.1445e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2155e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.2384e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1401e-04\n",
      "(1, 16780, 1)\n",
      "Epoch 1/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1071e-04\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1131e-04\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1308e-04\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1652e-04\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1532e-04\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1067e-04\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1140e-04\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1795e-04\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1894e-04\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1239e-04\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1726e-04\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.1754e-04\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2052e-04\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1250e-04\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 1.1129e-04\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1399e-04\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2473e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2575e-04\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1838e-04\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1859e-04\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1624e-04\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.1953e-04\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.2530e-04\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1990e-04\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1900e-04\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1510e-04\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 1.1214e-04\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1454e-04\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.1654e-04\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.0771e-04\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(reg_data.columns)):\n",
    "    sampl = batch_sample[i].reshape((1,batch_sample.shape[1],batch_sample.shape[2]))\n",
    "    print(sampl.shape)\n",
    "    labl = batch_label[i].reshape((batch_label.shape[1],batch_label.shape[2]))\n",
    "    model.fit(sampl,\n",
    "                          labl,\n",
    "                          epochs=epochs,\n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "URqDB0oxhX_X",
    "outputId": "35e8073b-7b4e-4098-f5e7-0f000124a0ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smoor\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00011356545292073861"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                 \"\"\"Testing with random interval(DeepAnT)\"\"\"\n",
    "# Set number of test sequences \n",
    "n_test_seq = 1\n",
    "\n",
    "# Split a univariate sequence into samples\n",
    "def generate_test_batch(raw_seq, n_test_seq):\n",
    "  # Sample a portion of the raw_seq randomly\n",
    "    ran_ix = random.randint(0,len(raw_seq) - n_test_seq * w - n_test_seq * p_w)\n",
    "    raw_test_seq = array(raw_seq[ran_ix:ran_ix + n_test_seq * w +  n_test_seq * p_w])\n",
    "    batch_test_seq, batch_test_label = list(), list()\n",
    "    ix = ran_ix\n",
    "    for i in range(n_test_seq):\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = raw_seq[ix : ix+w],\n",
    "        seq_y = raw_seq[ix+w : ix+w+p_w]\n",
    "        ix = ix+w+p_w\n",
    "        batch_test_seq.append(seq_x)\n",
    "        batch_test_label.append(seq_y)\n",
    "    return array(batch_test_seq), array(batch_test_label)\n",
    "\n",
    "batch_test_seq, batch_test_label = generate_test_batch(list(reg_data.ix[:,0]), n_test_seq)\n",
    "batch_test_seq = batch_test_seq.reshape((batch_test_seq.shape[0], w, n_features))\n",
    "batch_test_label = batch_test_label.reshape((batch_test_label.shape[0], p_w))\n",
    "\n",
    "# Returns the loss value & metrics values for the model in test mode\n",
    "model.evaluate(x=batch_test_seq,\n",
    "               y=batch_test_label,\n",
    "               verbose=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mK0_Cq-Oh0tC"
   },
   "outputs": [],
   "source": [
    "               \"\"\"Save Weights (DeepAnT)\"\"\"\n",
    "# save it to disk so we can load it back up anytime\n",
    "model.save_weights('ch_3_noanom_weights.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jpzknk4th9bU",
    "outputId": "ab1c4808-b829-4bd7-8d65-2e273a6e70b1"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange() (1,-17270, -17271)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-ebb5127f6183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Sample a portion of the raw_seq randomly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# 1. Choose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mran_ix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_seq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mp_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0minput_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mran_ix\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mran_ix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mtarget_seq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_seq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mran_ix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mran_ix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mp_w\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\random.py\u001b[0m in \u001b[0;36mrandint\u001b[1;34m(self, a, b)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     def _randbelow(self, n, int=int, maxsize=1<<BPF, type=type,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\random.py\u001b[0m in \u001b[0;36mrandrange\u001b[1;34m(self, start, stop, step, _int)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mistart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"empty range for randrange() (%d,%d, %d)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mistart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mistop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[1;31m# Non-unit step argument supplied.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: empty range for randrange() (1,-17270, -17271)"
     ]
    }
   ],
   "source": [
    "            \"\"\"Predicting random intervals (DeepAnT)\"\"\"\n",
    "# Build model \n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=num_filt_1,\n",
    "                 kernel_size=kernel_size,\n",
    "                 strides=conv_strides,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 input_shape=(w, n_features)))\n",
    "model.add(MaxPooling1D(pool_size=pool_size_1)) \n",
    "model.add(Conv1D(filters=num_filt_2,\n",
    "                 kernel_size=kernel_size,\n",
    "                 strides=conv_strides,\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=pool_size_2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=num_nrn_dl, activation='relu')) \n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(units=num_nrn_ol))\n",
    "\n",
    "# Load the model's saved weights.\n",
    "model.load_weights('ch_1_weights.h5')\n",
    "          \n",
    "# Sample a portion of the raw_seq randomly\n",
    "# 1. Choose \n",
    "ran_ix = random.randint(1,len(raw_seq) - w - p_w)\n",
    "input_seq = array(raw_seq[ran_ix : ran_ix + w])\n",
    "target_seq = array(raw_seq[ran_ix + w : ran_ix + w + p_w])\n",
    "input_seq = input_seq.reshape((1, w, n_features))\n",
    "\n",
    "# Predict the next time stampes of the sampled sequence\n",
    "yhat = model.predict(input_seq, verbose=1)\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(yhat)\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(target_seq) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2-oM2iL1gw4Q",
    "outputId": "a8ac362e-fccd-4d6c-f946-386a27620206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 132ms/step\n",
      "[[ 3.82821381e-05  2.72709382e-04 -1.04611572e-04  1.75664289e-04\n",
      "  -2.63651193e-04  9.82765196e-05 -6.24680033e-05  2.18766218e-05\n",
      "  -6.01411630e-05 -1.44594640e-04  6.57765486e-06  1.48069361e-04\n",
      "  -1.32613553e-04  1.41890967e-04  1.28227730e-05  3.19012543e-05\n",
      "  -8.15251697e-05 -3.45380249e-05 -1.42565405e-05 -3.10365285e-04\n",
      "  -7.83923533e-05  2.21053240e-04 -8.12578000e-05 -7.77772439e-05\n",
      "   1.69939522e-04  4.36319533e-05 -1.86225618e-04  2.89960823e-04\n",
      "  -1.49035739e-04 -2.03572650e-04 -1.12929643e-04 -2.34548832e-04\n",
      "  -3.31907831e-05 -5.34707506e-05  3.82364553e-04 -8.47358897e-05\n",
      "   6.85656778e-05 -4.96993089e-05 -2.72858306e-04  1.96144974e-05\n",
      "   8.73349200e-06  1.88414124e-04 -2.04249562e-04 -2.72675243e-05\n",
      "   1.03251805e-05  8.31427242e-05 -1.36694172e-04 -1.29406224e-04\n",
      "  -8.01810893e-06 -1.70319501e-04 -1.17470539e-04 -1.85399404e-06\n",
      "   7.00943929e-07  3.66317872e-05 -1.27697946e-04  1.58869298e-04\n",
      "   2.07254430e-04  1.44733087e-04 -2.26725286e-04 -1.29900945e-06\n",
      "  -2.96283513e-04 -2.19023743e-04 -1.14090966e-04  1.87918020e-04\n",
      "   2.36816879e-04 -1.13047972e-04 -1.43154000e-04  5.04324889e-05\n",
      "  -3.02721455e-05 -1.22718280e-04  2.92592653e-04 -2.22397357e-04\n",
      "   8.01484566e-05  6.90898887e-05  1.59483447e-04 -2.99557723e-05\n",
      "  -8.85373156e-05  1.37248906e-04 -2.91329634e-05  2.94124009e-04\n",
      "   8.75912428e-06  1.74659857e-04 -4.83963959e-05 -3.39685939e-05\n",
      "  -4.94365813e-05 -9.60184479e-05  1.69574021e-04  1.97794288e-05\n",
      "  -6.31655712e-05 -6.26815017e-06  2.40906375e-05  9.47947992e-05\n",
      "   1.13140050e-06 -2.49264631e-05  1.40272197e-04 -1.91703002e-04\n",
      "  -1.34551301e-05 -2.99257808e-05 -2.06573633e-04  1.18549600e-04\n",
      "   6.79513760e-05 -1.50931228e-04  1.11322151e-05 -2.39201487e-04\n",
      "   2.20848626e-04 -1.46542676e-04 -1.75260444e-04  1.25598977e-04\n",
      "   1.81157695e-04  1.09681409e-04  2.73324258e-06 -1.88689315e-04\n",
      "   1.64188910e-04 -9.84894286e-05 -1.93697546e-04  1.59032104e-04\n",
      "   2.36261021e-05  5.98997576e-05  5.99026644e-05  1.34989896e-05\n",
      "  -8.53799720e-05 -1.98103837e-04  1.95341097e-04 -9.62361810e-07\n",
      "  -2.87232251e-05 -1.43979545e-04 -6.43471663e-04  7.17218427e-05\n",
      "  -8.74619509e-05 -8.33882659e-05  4.38720599e-05  9.99178155e-07\n",
      "  -9.05590787e-05  1.58924086e-04  3.13377241e-05  3.76094249e-05\n",
      "  -8.35257233e-05 -1.40346427e-04  5.18015549e-05  7.67497258e-05\n",
      "  -1.85901983e-04  3.80461643e-05  4.54350666e-05  6.59850848e-05\n",
      "  -1.61874326e-04 -2.10951839e-06 -4.59528674e-05  7.99454720e-05\n",
      "  -5.90125055e-05 -1.85021985e-04 -7.61829506e-06 -1.79640483e-04\n",
      "  -2.31527956e-05  8.96184356e-05 -1.14845752e-05 -7.78924368e-05\n",
      "   5.58419197e-06  2.45721079e-04  3.20543186e-05 -9.57312295e-06\n",
      "  -8.01930073e-05 -4.90683306e-06 -1.82368403e-05  8.28629127e-05\n",
      "  -7.37720475e-05 -9.44957501e-05  7.75236913e-06 -2.86106020e-04\n",
      "   8.63856912e-05 -8.04600932e-05  4.37494047e-04  6.30500290e-06\n",
      "   8.91452801e-05 -1.36437229e-04  1.19874720e-04  7.37840310e-06\n",
      "  -1.04073015e-05  1.18202675e-04  1.33274705e-04 -4.47668135e-05\n",
      "   1.02635568e-04 -3.92763468e-06  4.80994640e-06 -1.31318433e-04\n",
      "   9.38232843e-05  1.88116610e-04 -2.47989490e-04  1.27778811e-04\n",
      "   8.10392085e-05 -7.30337488e-05  1.22937432e-04  4.35265174e-05\n",
      "  -1.21294746e-04  1.77529699e-04  1.14696915e-04  9.67265805e-08\n",
      "  -8.37131374e-05 -5.84260561e-05  1.84998193e-04  8.72859891e-05\n",
      "  -2.74269096e-05 -1.57871633e-04 -1.76427071e-04 -1.31693974e-04\n",
      "  -9.91065317e-05 -5.64803486e-06 -8.63348396e-05 -4.67433301e-06\n",
      "   2.28105673e-05 -8.73547251e-05  2.78266234e-04 -2.29048208e-04\n",
      "   2.63832393e-04  8.18232584e-05 -1.25460007e-04 -1.25774910e-04\n",
      "  -2.28505509e-04 -8.76994964e-05  1.68761617e-05 -1.39925469e-04\n",
      "  -6.11587457e-05  2.99777545e-04 -1.94178283e-04 -2.13014995e-04\n",
      "   1.08052351e-04  5.64011803e-04  4.65990306e-05  1.47394268e-04\n",
      "   6.38420388e-05  4.24466562e-05  8.17360633e-05 -1.37107825e-04\n",
      "  -3.01961554e-05  2.49030927e-05  4.05496336e-04 -1.39398762e-05\n",
      "  -8.59927241e-07 -3.65742802e-04  2.71251520e-06  3.48463451e-04\n",
      "  -5.77678547e-05 -9.29242105e-06  8.96444544e-05  1.07482985e-04\n",
      "  -1.08906861e-04 -1.37358686e-04  6.63101237e-05  3.65400279e-04\n",
      "   1.53213186e-04  1.36049232e-04  3.68894580e-05  2.81691719e-05\n",
      "   6.21781655e-05  1.98390626e-06 -1.06856038e-04  4.79491900e-05\n",
      "   1.78066854e-04  2.05774180e-04 -1.50996610e-04 -9.01332824e-05\n",
      "   6.42767991e-05  5.48645148e-05  9.55065916e-05 -8.01908609e-05\n",
      "   1.24229991e-04 -2.58649845e-04  1.96344772e-05  4.33979003e-05\n",
      "   2.22080867e-04 -1.88138816e-04 -2.45456409e-04 -2.90220458e-04\n",
      "  -7.56314839e-05 -2.42428039e-04  1.64066892e-04 -2.05394099e-06\n",
      "   1.74688015e-04  2.64126691e-04  7.56054942e-05 -3.39029575e-05\n",
      "   6.88206928e-05  6.71361850e-05  2.74687336e-04  1.53931076e-04\n",
      "   5.49293909e-05 -1.62520737e-05  1.07457614e-04  4.33293899e-05\n",
      "   6.15539757e-05  1.14654453e-04 -2.48668715e-04  1.23133752e-04\n",
      "  -1.14036211e-05 -2.32986567e-04 -1.05289088e-04  2.94859783e-04\n",
      "   1.14443064e-04  9.53146664e-05  1.81476862e-04 -3.36834928e-05\n",
      "   1.19372344e-04  1.15185845e-04 -3.36641551e-06  1.53050234e-04\n",
      "  -8.31291472e-05 -2.33099156e-04  1.66606653e-04  1.23192280e-04\n",
      "  -1.92016450e-05 -6.00204658e-07 -3.24474313e-05 -6.29877468e-05\n",
      "  -6.86863787e-05  1.58396651e-05 -1.34734233e-04 -5.37360138e-05\n",
      "  -3.61774146e-05  9.48493107e-06  1.00505749e-04  5.89198316e-06\n",
      "  -1.11219037e-04 -5.82202993e-05 -6.67365675e-05  2.01541014e-04\n",
      "  -1.68829371e-04  4.41807388e-05  1.03120256e-05 -8.31934303e-05\n",
      "   2.51447491e-04 -1.45090758e-04 -2.76143342e-04  2.07332108e-04\n",
      "   1.15986695e-04 -1.31153458e-04 -4.00712015e-05 -1.44079269e-04\n",
      "  -1.64755533e-04  4.34290378e-05 -2.62045360e-05 -1.43837184e-04\n",
      "   1.84015997e-04  3.62788996e-05 -1.49227722e-04  1.31384702e-04\n",
      "  -4.48165301e-05  4.54495224e-04  3.44491127e-07 -7.22327968e-07\n",
      "  -2.42658934e-04 -5.41424815e-05 -2.19472742e-04  7.74785149e-05\n",
      "   7.15798014e-05  3.34870711e-05  1.28565138e-04  1.79942319e-04\n",
      "  -3.22852356e-05  1.10335241e-05 -1.63496647e-04 -8.77584898e-06\n",
      "  -9.97514580e-05  1.09167740e-05 -1.51875967e-04  2.93119156e-05\n",
      "   2.12189916e-04 -5.98011247e-05  7.78848771e-05  1.63598394e-04\n",
      "   7.90567137e-05  8.41466608e-05 -8.94275727e-05 -2.15077875e-04\n",
      "   1.89822182e-04 -5.87214163e-05  1.02434424e-05  2.24107833e-04\n",
      "  -9.03454929e-06  4.34954010e-04 -1.77742972e-04 -2.59170018e-04\n",
      "   5.59227919e-06  2.51845864e-04 -2.86042341e-04  9.75825606e-05\n",
      "  -2.06482306e-04  4.20353572e-05  6.10759671e-05 -7.65711884e-05\n",
      "   1.12540627e-04  9.00196756e-06 -1.69453269e-04 -8.73098907e-05\n",
      "   8.82419117e-05 -1.31782916e-04  1.11981775e-04  1.08155182e-05\n",
      "   1.66472018e-05  8.67406270e-05  9.08158545e-05 -1.55634043e-04\n",
      "  -2.02641531e-05 -1.97675778e-04  2.46307172e-04  1.53749032e-04\n",
      "  -1.66533340e-04 -3.01741093e-05 -3.24007851e-06  3.09994095e-04\n",
      "   1.54759706e-04  1.85078796e-04  1.50832027e-04  1.47866362e-04\n",
      "   2.05916338e-04  7.69124745e-05 -7.54035864e-05  1.11652138e-04\n",
      "   3.00915981e-05 -2.44295734e-06  1.75847526e-05 -9.88778775e-05\n",
      "   1.05796593e-04  1.72074098e-04 -9.86109590e-05 -1.55243106e-04\n",
      "   5.95014753e-05 -5.96666650e-05 -1.43040394e-04 -1.52279288e-04\n",
      "  -1.36521703e-04 -2.85701302e-04  1.81169453e-05 -1.83097844e-04\n",
      "  -5.62458299e-05 -1.32057947e-04 -1.80548654e-04  7.28361410e-05\n",
      "  -7.67080928e-06 -2.26628428e-04 -1.66594691e-06  1.70008025e-05\n",
      "  -1.67908642e-04 -8.70586518e-05 -1.13466172e-04  8.86677080e-05\n",
      "  -4.56761263e-05 -1.35321068e-04 -1.30925706e-04 -1.67437047e-05\n",
      "   7.96845416e-05  1.76705245e-04 -6.57329510e-05 -1.21641497e-04\n",
      "   8.09369958e-05  4.08634369e-05  3.66356340e-04 -8.90827432e-05\n",
      "  -6.93280454e-05 -4.39358264e-05  1.70763349e-04  7.10967288e-06\n",
      "  -1.59999705e-04  1.05837215e-04 -9.72076305e-05  7.35961657e-05\n",
      "  -1.24428610e-04 -8.80262087e-05 -1.32454406e-05 -4.52362401e-05\n",
      "  -1.35699549e-04 -2.49673874e-04  1.58215262e-05  1.10067929e-04\n",
      "   1.47091778e-04  4.98455483e-05 -1.35474780e-04 -1.26792016e-04\n",
      "  -1.57015893e-05  1.51889370e-04 -1.52362569e-04  7.05925631e-05\n",
      "   9.56623699e-05  7.49810570e-05  9.25197673e-06 -4.05798433e-04\n",
      "  -2.99566367e-04 -1.66173530e-04  4.84966877e-05  1.63450270e-04\n",
      "  -1.74492801e-04 -6.09052004e-05 -2.79726111e-04  4.67173086e-05\n",
      "  -2.27641431e-04 -1.29690045e-04 -6.33891614e-05  3.74707961e-05\n",
      "   6.88995569e-05 -2.03295378e-04  1.26758154e-04 -5.36083098e-06]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smoor\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "            \"\"\"Predicting future sequence (DeepAnT)\"\"\"\n",
    "# Build model \n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=num_filt_1,\n",
    "                 kernel_size=kernel_size,\n",
    "                 strides=conv_strides,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 input_shape=(w, n_features)))\n",
    "model.add(MaxPooling1D(pool_size=pool_size_1)) \n",
    "model.add(Conv1D(filters=num_filt_2,\n",
    "                 kernel_size=kernel_size,\n",
    "                 strides=conv_strides,\n",
    "                 padding='valid',\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=pool_size_2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=num_nrn_dl, activation='relu')) \n",
    "model.add(Dropout(dropout_rate))\n",
    "model.add(Dense(units=num_nrn_ol))\n",
    "\n",
    "# Load the model's saved weights.\n",
    "model.load_weights('ch_1_weights.h5')\n",
    "          \n",
    "    \n",
    "raw_seq = list(reg_data.ix[:,0])\n",
    "endix = len(raw_seq) - w - p_w\n",
    "input_seq = array(raw_seq[endix:endix+w])\n",
    "target_seq = array(raw_seq[endix+w:endix+w+p_w]) \n",
    "input_seq = input_seq.reshape((1, w, n_features))\n",
    "\n",
    "# Predict the next time stampes of the sampled sequence\n",
    "predicted_seq = model.predict(input_seq, verbose=1)\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(predicted_seq)\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(target_seq) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "664VO9gMmmFE",
    "outputId": "0f8b7983-5c1c-4b94-d898-865d8f0fc0fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smoor\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n",
      "C:\\Users\\smoor\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\smoor\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7200x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "           '''Visualization of predicted time series'''\n",
    "in_seq = reg_data.ix[:,i][endix:endix+w]\n",
    "tar_seq = reg_data.ix[:,i][endix+w:endix+w+p_w]\n",
    "predicted_seq = predicted_seq.reshape((p_w))\n",
    "d = {'time': reg_data.ix[:,i][endix+w:endix+w+p_w], 'values': predicted_seq}\n",
    "df_sine_pre = pd.DataFrame(data=d)\n",
    "pre_seq = df_sine_pre['values']\n",
    "\n",
    "plt.plot(in_seq)\n",
    "plt.plot(tar_seq)\n",
    "plt.plot(pre_seq)\n",
    "plt.ylim(top=.05)\n",
    "plt.ylim(bottom=-.05)\n",
    "\n",
    "plt.title('Channel 3 Prediction')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('time')\n",
    "plt.legend(['input_seq', 'target_seq', 'pre_seq'], loc='upper right')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([endix,endix+w+p_w])\n",
    "fig_predict = plt.figure(figsize=(100,10))\n",
    "fig_predict.savefig('predicted_sequence.png')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "no2SqYbNoPXG",
    "outputId": "6925ffa8-9a85-4f10-99a9-f6dedb408878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\smoor\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/500\n",
      "5/5 [==============================] - 0s 60ms/step - loss: 17.2053\n",
      "Epoch 2/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 17.0966\n",
      "Epoch 3/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 16.9761\n",
      "Epoch 4/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 16.8562\n",
      "Epoch 5/500\n",
      "5/5 [==============================] - 0s 993us/step - loss: 16.7356\n",
      "Epoch 6/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 16.6270\n",
      "Epoch 7/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 16.5264\n",
      "Epoch 8/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 16.4241\n",
      "Epoch 9/500\n",
      "5/5 [==============================] - 0s 1000us/step - loss: 16.3190\n",
      "Epoch 10/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 16.2092\n",
      "Epoch 11/500\n",
      "5/5 [==============================] - 0s 607us/step - loss: 16.1016\n",
      "Epoch 12/500\n",
      "5/5 [==============================] - 0s 593us/step - loss: 15.9974\n",
      "Epoch 13/500\n",
      "5/5 [==============================] - 0s 397us/step - loss: 15.8933\n",
      "Epoch 14/500\n",
      "5/5 [==============================] - 0s 393us/step - loss: 15.7906\n",
      "Epoch 15/500\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 15.6849\n",
      "Epoch 16/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 15.5822\n",
      "Epoch 17/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 15.4773\n",
      "Epoch 18/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 15.3700\n",
      "Epoch 19/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 15.2653\n",
      "Epoch 20/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 15.1587\n",
      "Epoch 21/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 15.0502\n",
      "Epoch 22/500\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 14.9373\n",
      "Epoch 23/500\n",
      "5/5 [==============================] - 0s 603us/step - loss: 14.8196\n",
      "Epoch 24/500\n",
      "5/5 [==============================] - 0s 401us/step - loss: 14.6943\n",
      "Epoch 25/500\n",
      "5/5 [==============================] - 0s 603us/step - loss: 14.5655\n",
      "Epoch 26/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 14.5582\n",
      "Epoch 27/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 14.5307\n",
      "Epoch 28/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 14.4745\n",
      "Epoch 29/500\n",
      "5/5 [==============================] - 0s 392us/step - loss: 14.4126\n",
      "Epoch 30/500\n",
      "5/5 [==============================] - 0s 407us/step - loss: 14.3158\n",
      "Epoch 31/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 14.2145\n",
      "Epoch 32/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 14.1030\n",
      "Epoch 33/500\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 14.0130\n",
      "Epoch 34/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 13.9792\n",
      "Epoch 35/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 13.9313\n",
      "Epoch 36/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 13.8699\n",
      "Epoch 37/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 13.7957\n",
      "Epoch 38/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 13.7098\n",
      "Epoch 39/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 13.6142\n",
      "Epoch 40/500\n",
      "5/5 [==============================] - 0s 602us/step - loss: 13.6379\n",
      "Epoch 41/500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 13.6961\n",
      "Epoch 42/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 13.7051\n",
      "Epoch 43/500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 13.6554\n",
      "Epoch 44/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 13.5916\n",
      "Epoch 45/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 13.5411\n",
      "Epoch 46/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 13.4776\n",
      "Epoch 47/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 13.4005\n",
      "Epoch 48/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 13.4080\n",
      "Epoch 49/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 13.3998\n",
      "Epoch 50/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 13.3717\n",
      "Epoch 51/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 13.3256\n",
      "Epoch 52/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 13.3042\n",
      "Epoch 53/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 13.2937\n",
      "Epoch 54/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 13.2638\n",
      "Epoch 55/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 13.2164\n",
      "Epoch 56/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 13.1526\n",
      "Epoch 57/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 13.0745\n",
      "Epoch 58/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 13.0376\n",
      "Epoch 59/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 13.0512\n",
      "Epoch 60/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 13.0499\n",
      "Epoch 61/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 13.0284\n",
      "Epoch 62/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 12.9891\n",
      "Epoch 63/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 12.9335\n",
      "Epoch 64/500\n",
      "5/5 [==============================] - 0s 598us/step - loss: 12.8635\n",
      "Epoch 65/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 12.8405\n",
      "Epoch 66/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 12.8000\n",
      "Epoch 67/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 12.7635\n",
      "Epoch 68/500\n",
      "5/5 [==============================] - 0s 3ms/step - loss: 12.7504\n",
      "Epoch 69/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 12.7163\n",
      "Epoch 70/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 12.6628\n",
      "Epoch 71/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 12.5927\n",
      "Epoch 72/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 12.5598\n",
      "Epoch 73/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 12.5380\n",
      "Epoch 74/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 12.5204\n",
      "Epoch 75/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 12.4844\n",
      "Epoch 76/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 12.4319\n",
      "Epoch 77/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 12.3730\n",
      "Epoch 78/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 12.3187\n",
      "Epoch 79/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 12.2883\n",
      "Epoch 80/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 12.2496\n",
      "Epoch 81/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 12.2030\n",
      "Epoch 82/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 12.1638\n",
      "Epoch 83/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 12.1158\n",
      "Epoch 84/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 12.0658\n",
      "Epoch 85/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 12.0553\n",
      "Epoch 86/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 11.9779\n",
      "Epoch 87/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 11.9727\n",
      "Epoch 88/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 11.9577\n",
      "Epoch 89/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 11.9122\n",
      "Epoch 90/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 11.8390\n",
      "Epoch 91/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 11.7464\n",
      "Epoch 92/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 11.7082\n",
      "Epoch 93/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 11.6818\n",
      "Epoch 94/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 11.6422\n",
      "Epoch 95/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 11.5736\n",
      "Epoch 96/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 11.5734\n",
      "Epoch 97/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 11.5029\n",
      "Epoch 98/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 11.4797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 11.4383\n",
      "Epoch 100/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 11.3941\n",
      "Epoch 101/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 11.3405\n",
      "Epoch 102/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 11.2914\n",
      "Epoch 103/500\n",
      "5/5 [==============================] - 0s 402us/step - loss: 11.2523\n",
      "Epoch 104/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 11.2085\n",
      "Epoch 105/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 11.1756\n",
      "Epoch 106/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 11.1256\n",
      "Epoch 107/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 11.0851\n",
      "Epoch 108/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 11.0343\n",
      "Epoch 109/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 10.9816\n",
      "Epoch 110/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.9293\n",
      "Epoch 111/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.8909\n",
      "Epoch 112/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.8347\n",
      "Epoch 113/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 10.7839\n",
      "Epoch 114/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.7418\n",
      "Epoch 115/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.6785\n",
      "Epoch 116/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 10.6541\n",
      "Epoch 117/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.6045\n",
      "Epoch 118/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.5355\n",
      "Epoch 119/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.4735\n",
      "Epoch 120/500\n",
      "5/5 [==============================] - 0s 401us/step - loss: 10.4210\n",
      "Epoch 121/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.3642\n",
      "Epoch 122/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.3116\n",
      "Epoch 123/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 10.2483\n",
      "Epoch 124/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 10.1974\n",
      "Epoch 125/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 10.1304\n",
      "Epoch 126/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 10.0791\n",
      "Epoch 127/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 10.0266\n",
      "Epoch 128/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 9.9614\n",
      "Epoch 129/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 9.8853\n",
      "Epoch 130/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 9.8194\n",
      "Epoch 131/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 9.7534\n",
      "Epoch 132/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 9.6822\n",
      "Epoch 133/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 9.6192\n",
      "Epoch 134/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 9.5430\n",
      "Epoch 135/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 9.4686\n",
      "Epoch 136/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 9.3865\n",
      "Epoch 137/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 9.3227\n",
      "Epoch 138/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 9.2440\n",
      "Epoch 139/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 9.1683\n",
      "Epoch 140/500\n",
      "5/5 [==============================] - 0s 401us/step - loss: 9.0809\n",
      "Epoch 141/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 9.0142\n",
      "Epoch 142/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 8.9331\n",
      "Epoch 143/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 8.8414\n",
      "Epoch 144/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 8.7638\n",
      "Epoch 145/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 8.6742\n",
      "Epoch 146/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 8.6017\n",
      "Epoch 147/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 8.4985\n",
      "Epoch 148/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 8.4409\n",
      "Epoch 149/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 8.3515\n",
      "Epoch 150/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 8.2410\n",
      "Epoch 151/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 8.1567\n",
      "Epoch 152/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 8.0635\n",
      "Epoch 153/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 7.9537\n",
      "Epoch 154/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 7.8649\n",
      "Epoch 155/500\n",
      "5/5 [==============================] - 0s 1000us/step - loss: 7.7595\n",
      "Epoch 156/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 7.6628\n",
      "Epoch 157/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 7.5473\n",
      "Epoch 158/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 7.4704\n",
      "Epoch 159/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 7.3782\n",
      "Epoch 160/500\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 7.2698\n",
      "Epoch 161/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 7.1428\n",
      "Epoch 162/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 7.0057\n",
      "Epoch 163/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 6.8882\n",
      "Epoch 164/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 6.7987\n",
      "Epoch 165/500\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 6.6596\n",
      "Epoch 166/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 6.5392\n",
      "Epoch 167/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 6.4464\n",
      "Epoch 168/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 6.3285\n",
      "Epoch 169/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 6.2317\n",
      "Epoch 170/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 6.1117\n",
      "Epoch 171/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 5.9709\n",
      "Epoch 172/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 5.8111\n",
      "Epoch 173/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 5.8159\n",
      "Epoch 174/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 5.5678\n",
      "Epoch 175/500\n",
      "5/5 [==============================] - 0s 201us/step - loss: 5.4750\n",
      "Epoch 176/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 5.3656\n",
      "Epoch 177/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 5.2314\n",
      "Epoch 178/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 5.0762\n",
      "Epoch 179/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 4.9417\n",
      "Epoch 180/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 4.8500\n",
      "Epoch 181/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 4.6877\n",
      "Epoch 182/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 4.5375\n",
      "Epoch 183/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 4.4358\n",
      "Epoch 184/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 4.3637\n",
      "Epoch 185/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 4.1942\n",
      "Epoch 186/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 4.0479\n",
      "Epoch 187/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 4.0144\n",
      "Epoch 188/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 3.8239\n",
      "Epoch 189/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 3.7314\n",
      "Epoch 190/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 3.5965\n",
      "Epoch 191/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 3.6480\n",
      "Epoch 192/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 3.5731\n",
      "Epoch 193/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 3.4655\n",
      "Epoch 194/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 3.4285\n",
      "Epoch 195/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 3.3770\n",
      "Epoch 196/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 3.3415\n",
      "Epoch 197/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 3.2333\n",
      "Epoch 198/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 3.1555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 3.0994\n",
      "Epoch 200/500\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.0309\n",
      "Epoch 201/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.9497\n",
      "Epoch 202/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.9218\n",
      "Epoch 203/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.8714\n",
      "Epoch 204/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 2.7751\n",
      "Epoch 205/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.7752\n",
      "Epoch 206/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.6831\n",
      "Epoch 207/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.5821\n",
      "Epoch 208/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.5369\n",
      "Epoch 209/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 2.4506\n",
      "Epoch 210/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.4100\n",
      "Epoch 211/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.3620\n",
      "Epoch 212/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.2840\n",
      "Epoch 213/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 2.2291\n",
      "Epoch 214/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.1919\n",
      "Epoch 215/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 2.1486\n",
      "Epoch 216/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.1008\n",
      "Epoch 217/500\n",
      "5/5 [==============================] - 0s 401us/step - loss: 2.0923\n",
      "Epoch 218/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0916\n",
      "Epoch 219/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0603\n",
      "Epoch 220/500\n",
      "5/5 [==============================] - 0s 401us/step - loss: 2.0671\n",
      "Epoch 221/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.1165\n",
      "Epoch 222/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 2.0979\n",
      "Epoch 223/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 2.0644\n",
      "Epoch 224/500\n",
      "5/5 [==============================] - 0s 401us/step - loss: 2.0854\n",
      "Epoch 225/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0439\n",
      "Epoch 226/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0779\n",
      "Epoch 227/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 2.0981\n",
      "Epoch 228/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.0899\n",
      "Epoch 229/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0586\n",
      "Epoch 230/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 2.0045\n",
      "Epoch 231/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0912\n",
      "Epoch 232/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.9731\n",
      "Epoch 233/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0713\n",
      "Epoch 234/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0544\n",
      "Epoch 235/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9872\n",
      "Epoch 236/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.9414\n",
      "Epoch 237/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.9414\n",
      "Epoch 238/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9862\n",
      "Epoch 239/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9602\n",
      "Epoch 240/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.9959\n",
      "Epoch 241/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9564\n",
      "Epoch 242/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9611\n",
      "Epoch 243/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.9369\n",
      "Epoch 244/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9646\n",
      "Epoch 245/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8994\n",
      "Epoch 246/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0111\n",
      "Epoch 247/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9114\n",
      "Epoch 248/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.1099\n",
      "Epoch 249/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0535\n",
      "Epoch 250/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.9640\n",
      "Epoch 251/500\n",
      "5/5 [==============================] - 0s 401us/step - loss: 1.9541\n",
      "Epoch 252/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0117\n",
      "Epoch 253/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 2.0281\n",
      "Epoch 254/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 1.9677\n",
      "Epoch 255/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.0801\n",
      "Epoch 256/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.9172\n",
      "Epoch 257/500\n",
      "5/5 [==============================] - 0s 401us/step - loss: 1.9575\n",
      "Epoch 258/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.9326\n",
      "Epoch 259/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9239\n",
      "Epoch 260/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0615\n",
      "Epoch 261/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9801\n",
      "Epoch 262/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0386\n",
      "Epoch 263/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0652\n",
      "Epoch 264/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.8577\n",
      "Epoch 265/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9532\n",
      "Epoch 266/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.9572\n",
      "Epoch 267/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.8912\n",
      "Epoch 268/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 2.0003\n",
      "Epoch 269/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9542\n",
      "Epoch 270/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.9958\n",
      "Epoch 271/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.0213\n",
      "Epoch 272/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 1.8598\n",
      "Epoch 273/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8911\n",
      "Epoch 274/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.9245\n",
      "Epoch 275/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9216\n",
      "Epoch 276/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 1.9050\n",
      "Epoch 277/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9518\n",
      "Epoch 278/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.0388\n",
      "Epoch 279/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.9256\n",
      "Epoch 280/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9392\n",
      "Epoch 281/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 1.9996\n",
      "Epoch 282/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.8610\n",
      "Epoch 283/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9287\n",
      "Epoch 284/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.7963\n",
      "Epoch 285/500\n",
      "5/5 [==============================] - 0s 398us/step - loss: 1.9056\n",
      "Epoch 286/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.8307\n",
      "Epoch 287/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8878\n",
      "Epoch 288/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8995\n",
      "Epoch 289/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7740\n",
      "Epoch 290/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.9798\n",
      "Epoch 291/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7769\n",
      "Epoch 292/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.8937\n",
      "Epoch 293/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.9759\n",
      "Epoch 294/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.8487\n",
      "Epoch 295/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8535\n",
      "Epoch 296/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7954\n",
      "Epoch 297/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8291\n",
      "Epoch 298/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7315\n",
      "Epoch 300/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.8946\n",
      "Epoch 301/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.7569\n",
      "Epoch 302/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.9396\n",
      "Epoch 303/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.0240\n",
      "Epoch 304/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.9041\n",
      "Epoch 305/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7803\n",
      "Epoch 306/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.8601\n",
      "Epoch 307/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7428\n",
      "Epoch 308/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8862\n",
      "Epoch 309/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.9450\n",
      "Epoch 310/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8217\n",
      "Epoch 311/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 1.7299\n",
      "Epoch 312/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.0278\n",
      "Epoch 313/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8272\n",
      "Epoch 314/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8901\n",
      "Epoch 315/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.0712\n",
      "Epoch 316/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 2.0489\n",
      "Epoch 317/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8608\n",
      "Epoch 318/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 1.8612\n",
      "Epoch 319/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 1.9997\n",
      "Epoch 320/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.9376\n",
      "Epoch 321/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7506\n",
      "Epoch 322/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.9409\n",
      "Epoch 323/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 2.0280\n",
      "Epoch 324/500\n",
      "5/5 [==============================] - 0s 397us/step - loss: 1.9347\n",
      "Epoch 325/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 1.7603\n",
      "Epoch 326/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0258\n",
      "Epoch 327/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9830\n",
      "Epoch 328/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7774\n",
      "Epoch 329/500\n",
      "5/5 [==============================] - 0s 1000us/step - loss: 1.9161\n",
      "Epoch 330/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.9254\n",
      "Epoch 331/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7598\n",
      "Epoch 332/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8561\n",
      "Epoch 333/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 2.0098\n",
      "Epoch 334/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.8025\n",
      "Epoch 335/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8244\n",
      "Epoch 336/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.9449\n",
      "Epoch 337/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.8817\n",
      "Epoch 338/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7284\n",
      "Epoch 339/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7537\n",
      "Epoch 340/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6827\n",
      "Epoch 341/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7915\n",
      "Epoch 342/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8114\n",
      "Epoch 343/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.7400\n",
      "Epoch 344/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.8324\n",
      "Epoch 345/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8532\n",
      "Epoch 346/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.7333\n",
      "Epoch 347/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.8569\n",
      "Epoch 348/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.8343\n",
      "Epoch 349/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.6829\n",
      "Epoch 350/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.8845\n",
      "Epoch 351/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9685\n",
      "Epoch 352/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.8642\n",
      "Epoch 353/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.6953\n",
      "Epoch 354/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8349\n",
      "Epoch 355/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.8461\n",
      "Epoch 356/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6986\n",
      "Epoch 357/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7940\n",
      "Epoch 358/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7341\n",
      "Epoch 359/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7722\n",
      "Epoch 360/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.8833\n",
      "Epoch 361/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.8234\n",
      "Epoch 362/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6684\n",
      "Epoch 363/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8472\n",
      "Epoch 364/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.7467\n",
      "Epoch 365/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.6682\n",
      "Epoch 366/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7166\n",
      "Epoch 367/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6751\n",
      "Epoch 368/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6855\n",
      "Epoch 369/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6605\n",
      "Epoch 370/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6873\n",
      "Epoch 371/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7562\n",
      "Epoch 372/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6961\n",
      "Epoch 373/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6500\n",
      "Epoch 374/500\n",
      "5/5 [==============================] - 0s 603us/step - loss: 1.8434\n",
      "Epoch 375/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.6636\n",
      "Epoch 376/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7812\n",
      "Epoch 377/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8706\n",
      "Epoch 378/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.7924\n",
      "Epoch 379/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.6711\n",
      "Epoch 380/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7541\n",
      "Epoch 381/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.7040\n",
      "Epoch 382/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.6546\n",
      "Epoch 383/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.7079\n",
      "Epoch 384/500\n",
      "5/5 [==============================] - 0s 401us/step - loss: 1.6719\n",
      "Epoch 385/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7587\n",
      "Epoch 386/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7312\n",
      "Epoch 387/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6469\n",
      "Epoch 388/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7006\n",
      "Epoch 389/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6512\n",
      "Epoch 390/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.7374\n",
      "Epoch 391/500\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.7548\n",
      "Epoch 392/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6846\n",
      "Epoch 393/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.7792\n",
      "Epoch 394/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.8070\n",
      "Epoch 395/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.6790\n",
      "Epoch 396/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.7051\n",
      "Epoch 397/500\n",
      "5/5 [==============================] - 0s 398us/step - loss: 1.6720\n",
      "Epoch 398/500\n",
      "5/5 [==============================] - 0s 398us/step - loss: 1.6287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6448\n",
      "Epoch 400/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.6052\n",
      "Epoch 401/500\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.7944\n",
      "Epoch 402/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5939\n",
      "Epoch 403/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6558\n",
      "Epoch 404/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6429\n",
      "Epoch 405/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6307\n",
      "Epoch 406/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6345\n",
      "Epoch 407/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6098\n",
      "Epoch 408/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 1.6206\n",
      "Epoch 409/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5918\n",
      "Epoch 410/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5798\n",
      "Epoch 411/500\n",
      "5/5 [==============================] - 0s 799us/step - loss: 1.5767\n",
      "Epoch 412/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5946\n",
      "Epoch 413/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5926\n",
      "Epoch 414/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5820\n",
      "Epoch 415/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.5665\n",
      "Epoch 416/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5711\n",
      "Epoch 417/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6128\n",
      "Epoch 418/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6326\n",
      "Epoch 419/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.7035\n",
      "Epoch 420/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.6190\n",
      "Epoch 421/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7115\n",
      "Epoch 422/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6022\n",
      "Epoch 423/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6510\n",
      "Epoch 424/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.6784\n",
      "Epoch 425/500\n",
      "5/5 [==============================] - 0s 449us/step - loss: 1.5538\n",
      "Epoch 426/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.7973\n",
      "Epoch 427/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6958\n",
      "Epoch 428/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6827\n",
      "Epoch 429/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.8337\n",
      "Epoch 430/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7384\n",
      "Epoch 431/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6123\n",
      "Epoch 432/500\n",
      "5/5 [==============================] - 0s 401us/step - loss: 1.8054\n",
      "Epoch 433/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6345\n",
      "Epoch 434/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 1.7463\n",
      "Epoch 435/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.9370\n",
      "Epoch 436/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.8759\n",
      "Epoch 437/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6337\n",
      "Epoch 438/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.8544\n",
      "Epoch 439/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9601\n",
      "Epoch 440/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 1.6284\n",
      "Epoch 441/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.8285\n",
      "Epoch 442/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 2.0714\n",
      "Epoch 443/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 2.0609\n",
      "Epoch 444/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8287\n",
      "Epoch 445/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5868\n",
      "Epoch 446/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.9430\n",
      "Epoch 447/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 1.9911\n",
      "Epoch 448/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6641\n",
      "Epoch 449/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7132\n",
      "Epoch 450/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.8913\n",
      "Epoch 451/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.8443\n",
      "Epoch 452/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.6469\n",
      "Epoch 453/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7145\n",
      "Epoch 454/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.9149\n",
      "Epoch 455/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7090\n",
      "Epoch 456/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.6733\n",
      "Epoch 457/500\n",
      "5/5 [==============================] - 0s 403us/step - loss: 1.8736\n",
      "Epoch 458/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.8817\n",
      "Epoch 459/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.7507\n",
      "Epoch 460/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5890\n",
      "Epoch 461/500\n",
      "5/5 [==============================] - 0s 199us/step - loss: 1.7734\n",
      "Epoch 462/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6783\n",
      "Epoch 463/500\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.6383\n",
      "Epoch 464/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7772\n",
      "Epoch 465/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.7769\n",
      "Epoch 466/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6542\n",
      "Epoch 467/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5834\n",
      "Epoch 468/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.6381\n",
      "Epoch 469/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5595\n",
      "Epoch 470/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5661\n",
      "Epoch 471/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5734\n",
      "Epoch 472/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5705\n",
      "Epoch 473/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5552\n",
      "Epoch 474/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5792\n",
      "Epoch 475/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5344\n",
      "Epoch 476/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5895\n",
      "Epoch 477/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5895\n",
      "Epoch 478/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5843\n",
      "Epoch 479/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 1.6052\n",
      "Epoch 480/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5567\n",
      "Epoch 481/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5542\n",
      "Epoch 482/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5579\n",
      "Epoch 483/500\n",
      "5/5 [==============================] - 0s 601us/step - loss: 1.5940\n",
      "Epoch 484/500\n",
      "5/5 [==============================] - 0s 801us/step - loss: 1.5743\n",
      "Epoch 485/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5987\n",
      "Epoch 486/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5479\n",
      "Epoch 487/500\n",
      "5/5 [==============================] - 0s 599us/step - loss: 1.5462\n",
      "Epoch 488/500\n",
      "5/5 [==============================] - 0s 603us/step - loss: 1.5477\n",
      "Epoch 489/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.5384\n",
      "Epoch 490/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5457\n",
      "Epoch 491/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5026\n",
      "Epoch 492/500\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.4924\n",
      "Epoch 493/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.4956\n",
      "Epoch 494/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.5062\n",
      "Epoch 495/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.4951\n",
      "Epoch 496/500\n",
      "5/5 [==============================] - 0s 399us/step - loss: 1.4913\n",
      "Epoch 497/500\n",
      "5/5 [==============================] - 0s 400us/step - loss: 1.5052\n",
      "Epoch 498/500\n",
      "5/5 [==============================] - 0s 600us/step - loss: 1.4875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499/500\n",
      "5/5 [==============================] - 0s 200us/step - loss: 1.4914\n",
      "Epoch 500/500\n",
      "5/5 [==============================] - 0s 800us/step - loss: 1.4928\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "[[ 93.776695 -81.86888 ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFv7f2x-euPk"
   },
   "outputs": [],
   "source": [
    "               \"\"\"Save Weights (ShallowAnT)\"\"\"\n",
    "# save it to disk so we can load it back up anytime\n",
    "model.save_weights('shallow_ch_1_weights.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Determins whether a sequence exceeds the threshold for being an anomaly\n",
    "\n",
    "return boolean value of whether the sequence is an anomaly or not\n",
    "\"\"\"\n",
    "def anomaly_detector(prediction_seq, ground_truth_seq):\n",
    "    # calculate Euclidean between actual seq and predicted seq\n",
    "    dist = np.linalg.norm(ground_truth_seq - prediction_seq)  \n",
    "    if (dist > anm_det_thr):\n",
    "        return true  # anomaly\n",
    "    else:\n",
    "        return false # normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "include_colab_link": true,
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda6f2bec422dd747988837f1b049720b89"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
